base_model: mistralai/Mistral-7B-Instruct-v0.2
load_in_8bit: false
load_in_4bit: true
strict: false

datasets:
  - path: data/lora/sections.jsonl
    type: json
    field_instruction: instruction
    field_output: output
dataset_prepared_path: ./data/axolotl-prepared

output_dir: outputs/ax_mistral_sections
save_steps: 500
logging_steps: 20
eval_steps: 0
num_train_epochs: 3
learning_rate: 2e-4
adam_beta1: 0.9
adam_beta2: 0.999
weight_decay: 0.0
lr_scheduler: cosine
warmup_ratio: 0.03

trainer: lora
gradient_accumulation_steps: 32
per_device_train_batch_size: 1
max_seq_length: 4096

flash_attention: auto
fsdp: none

peft:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

gradient_checkpointing: true
bitsandbytes:
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true

wandb_project: content-style-lora
warmup_steps: 100
